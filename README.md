# TinyML

This course is part of the Verizon Skill Forward program. Thank you to Verizon for providing the full course experience for self-learners like me.

## Introduction
* TinyML is concerned with creating intelligent systems on microcontrollers.
* Microcontrollers are cheap, energy efficient, and resource contrained devices.
* Big Picture: Generate predictions from data generated by external sensors (i.e. cameras, pressure, temperature, etc.).

### Hardware and Software Challenges
Hardware | Software 
-|-
Limited in terms of performance, power consumption, and storage | Not as portable as mainstream software

### Floating Point Numbers on Embedded Systems
Before, floating point operations were handled via software libraries, but they were slow. Now, with Floating Point Units (FPUs), which are dedicated hardware processors to perform operations on floating point numbers, operating on floating point numbers are much faster. However, if you write code optimized for a specific FPU, it may not work on other FPUs, as they may have different architectures or instruction sets.

### Embedded Machine Learning Software Challenges
Larger models require more computational power but an embedded systems can only offer so much it. So, we need to find a way to fit these large models into our microcontrollers while keeping the restrictions in mind. Here are some ways of doing so:
* Model Compression: Pruning - Removing some nodes and connections while still producing the same results.
* Model Compression: Quantization - Reducing the size of the numbers we're working with to take up less memory space.
* Model Compression: Knowledge Distillation - Only keep the most critical pieces of information to produce our results.
* Reducing Runtimes: Making inferences from our pre-made model instead of learning from new data

## Basic Machine Learning
* Machine Learning Paradigm: Labeled data is used to create a set of rules (models) that are used to generate inferences (probabilities) from future data.
* Measuring loss in our model via Mean Squared Error (MSE): Take the average of the squared differences between Y (model generated output) and Y' (real output). The lower the error, the better our model. Check out a [code sample](mse.ipynb).
* Minimizing loss in our model via Gradient Descent: Imagine plotting our loss function. The y-axis would be the error from our loss function (i.e. MSE) and our x-axis would be one of our parameters. If you have more than one parameter, you would have several axes. You want to get the minimum point in that graph. Gradient descent allows us to adjust our parameters via a learning rate in order to reach that minimum. Too large or too small of a learning rate may result in never reaching a minimum. Check out a [code sample](gradient-descent.ipynb).
* As our data becomes more complex, a simple mathematical model may not be enough to capture these complicated patterns. So, we can use a neural network. Neural networks consists of neurons that have their 'own' little model that generates an output from provided parameters. These little models are called neurons. Here is an [example](neural-network.ipynb) of a single neuron that performs linear regression. We can have a lot of these neurons working together (an output of one neuron can be an input to another neuron). 
